<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Cool Blog</title>
    <link>/</link>
    <description>Recent content on My Cool Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Some Exercises, Part 1</title>
      <link>/2018/10/25/some-exercises-part-1/</link>
      <pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/25/some-exercises-part-1/</guid>
      <description>Introduction In this post I will work through a couple of exercies from “Cracking the Coding Interview” by Gayle McDowell.
 1.1 Implement an algorithm to determine if a string has all unique characters.
In R this is accomplished with the following code:
all_unique_chars = function(string){ try(!any(table(strsplit(string,&amp;quot;&amp;quot;)[[1]]) &amp;gt; 1)) } library(testthat) test_that(&amp;quot;Test when there are duplicated characters&amp;quot;, expect_equal(all_unique_chars(&amp;quot;foobar&amp;quot;),FALSE)) test_that(&amp;quot;Test when there are NO duplicated characters&amp;quot;, expect_equal(all_unique_chars(&amp;quot;big&amp;quot;),TRUE)) The main idea here is that the table function creates a contingency table, after which we check if any value in the table is greater than 1, signaling that that character is not unique.</description>
    </item>
    
    <item>
      <title>An Implementation of K-means&#43;&#43;, Part 1</title>
      <link>/2018/10/17/an-implementation-of-k-means/</link>
      <pubDate>Wed, 17 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/17/an-implementation-of-k-means/</guid>
      <description>Introduction Previously I wrote about K-means clustering using Lloyd’s Algorithm. That algorithm can be split into two parts:
Choosing initial centers Iterating until final centers are chosen  I mostly ignored step 1 by choosing centers randomly from the available data. However, there are other ways to choose centers, and choosing centers randomly can lead to various problems. For example, the number of steps to convergence could be much larger if the initial centers are very closer together than if the initial centers were closer to their final positions.</description>
    </item>
    
    <item>
      <title>Why Does Lloyd&#39;s Algorithm Converge?</title>
      <link>/2018/10/11/why-does-lloyd-s-algorithm-converge/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/11/why-does-lloyd-s-algorithm-converge/</guid>
      <description>Introduction In the last post I presented an implementation of Lloyd’s Algorithm for solving K-means clustering. One question I did not answer there is this: why does Lloyd’s Algorithm work? Why should it converge at all? In this post I will attempt to answer this question.
 Cost Function K-means minimizes the within cluster sums of squares. We can define a function to describe this cost:
library(tidyverse) # point and mu are numerical vectors of the same length sum_of_squares = function(point,mu){ sum((point - mu)^2) } # points a data.</description>
    </item>
    
    <item>
      <title>An Implementation of K-means (Lloyd&#39;s Algorithm)</title>
      <link>/2018/10/01/an-implementation-of-k-means/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/01/an-implementation-of-k-means/</guid>
      <description>Introduction In this post I will write my own implementation of K-means. K-means is a popular clustering algorithm. You can read about it here: https://en.wikipedia.org/wiki/K-means_clustering.
 Algorithm Here is the description of the steps of the algorithm:
Choose k initial centers (or ‘centroids’). This can be done in many ways: chosen randomly from random uniform distributions, chosen randomly from the data, and other methods including k-means++, which I will discuss in a future post.</description>
    </item>
    
    <item>
      <title>My first post</title>
      <link>/2018/09/26/my-first-post/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/26/my-first-post/</guid>
      <description>My cool blog post.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>Welcome to my blog. I&amp;rsquo;m an R programmer and I post about statistical algorithms that interest me.</description>
    </item>
    
  </channel>
</rss>