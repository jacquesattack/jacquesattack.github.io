<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on My Cool Blog</title>
    <link>/post/</link>
    <description>Recent content in Posts on My Cool Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Minkowski Distance</title>
      <link>/2018/11/08/minkowski-distance/</link>
      <pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/08/minkowski-distance/</guid>
      <description>I was perusing this chapter on recommendation systems when I read about Minkowski distance and its relation to Manhattan distance and Euclidean distance. There is an interesting way to use R’s functional aspect to create a “distance factory” based on Minkowski distance:
library(pracma) distance_factory = function(r){ if(r &amp;lt;= 0) stop(&amp;quot;argument to function must be positive&amp;quot;) function(x,y){ nthroot(sum(abs(x-y)^r),r) } } And now we can create different types of distance functions from the factory:</description>
    </item>
    
    <item>
      <title>An Implementation of K-means||</title>
      <link>/2018/11/03/an-implementation-of-k-means/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/03/an-implementation-of-k-means/</guid>
      <description>Introduction Although K-means++ provides good initial centers as seeds for the K-means (Lloyd’s) algorithm, it has a drawback: the centers are chosen sequentially. In the era of big data, every pass through the data could be expensive. Bahmani et. al. have written a parallel implementation of K-means++ which they discuss here and which I will implement in this post.
 Algorithm library(tidyverse) dist = function(v1,v2) { sqrt(sum((v1-v2)^2)) } # point and mu are numerical vectors of the same length sum_of_squares = function(point,mu){ sum((point - mu)^2) } # points a data.</description>
    </item>
    
    <item>
      <title>Some Exercises, Part 2</title>
      <link>/2018/10/28/some-exercises-part-2/</link>
      <pubDate>Sun, 28 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/28/some-exercises-part-2/</guid>
      <description>1.3 Write a method to replace all spaces in a string with ‘%20’.
Of course you can ‘cheat’ and just use gsub:
input = &amp;quot;John Smith&amp;quot; print(gsub(&amp;quot; &amp;quot;,&amp;quot;%20&amp;quot;,input)) ## [1] &amp;quot;John%20Smith&amp;quot; Let’s try to write it ourselves. Here I use a useful pattern in R, which is preallocating a list object in memory and then calling do.call at the end. This is much better than calling c at every stage, or any other function.</description>
    </item>
    
    <item>
      <title>An Implementation of K-means&#43;&#43;, Part 2</title>
      <link>/2018/10/27/an-implementation-of-k-means-part-2/</link>
      <pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/27/an-implementation-of-k-means-part-2/</guid>
      <description>Introduction In Part 1 of K-means++, I discussed the motivation for this algorithm and made some changes to the original K-means implementation. Now I will write the actual implementation.
 Algorithm library(tidyverse) library(magrittr) dist = function(v1,v2) { sqrt(sum((v1-v2)^2)) } kpp = function(data,k) { centroids = numeric(k) # assign first centroid at random from points centroids[1] = sample(1:nrow(data),1) for(i in 2:k) { # For each data point x, compute D(x), the distance between # x and the nearest center that has already been chosen d_x = apply(data,1,function(row){ # find closest center d_to_centers = sapply(centroids[1:(i-1)], function(center) { dist(row,data[center,]) }) min(d_to_centers) }) # Choose one new data point at random as a new center, # using a weighted probability distribution where a point x # is chosen with probability proportional to D(x)^2.</description>
    </item>
    
    <item>
      <title>Some Exercises, Part 1</title>
      <link>/2018/10/25/some-exercises-part-1/</link>
      <pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/25/some-exercises-part-1/</guid>
      <description>Introduction In this post I will work through a couple of exercies from “Cracking the Coding Interview” by Gayle McDowell.
 1.1 Implement an algorithm to determine if a string has all unique characters.
In R this is accomplished with the following code:
all_unique_chars = function(string){ try(!any(table(strsplit(string,&amp;quot;&amp;quot;)[[1]]) &amp;gt; 1)) } library(testthat) test_that(&amp;quot;Test when there are duplicated characters&amp;quot;, expect_equal(all_unique_chars(&amp;quot;foobar&amp;quot;),FALSE)) test_that(&amp;quot;Test when there are NO duplicated characters&amp;quot;, expect_equal(all_unique_chars(&amp;quot;big&amp;quot;),TRUE)) The main idea here is that the table function creates a contingency table, after which we check if any value in the table is greater than 1, signaling that that character is not unique.</description>
    </item>
    
    <item>
      <title>An Implementation of K-means&#43;&#43;, Part 1</title>
      <link>/2018/10/17/an-implementation-of-k-means/</link>
      <pubDate>Wed, 17 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/17/an-implementation-of-k-means/</guid>
      <description>Introduction Previously I wrote about K-means clustering using Lloyd’s Algorithm. That algorithm can be split into two parts:
Choosing initial centers Iterating until final centers are chosen  I mostly ignored step 1 by choosing centers randomly from the available data. However, there are other ways to choose centers, and choosing centers randomly can lead to various problems. For example, the number of steps to convergence could be much larger if the initial centers are very closer together than if the initial centers were closer to their final positions.</description>
    </item>
    
    <item>
      <title>Why Does Lloyd&#39;s Algorithm Converge?</title>
      <link>/2018/10/11/why-does-lloyd-s-algorithm-converge/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/11/why-does-lloyd-s-algorithm-converge/</guid>
      <description>Introduction In the last post I presented an implementation of Lloyd’s Algorithm for solving K-means clustering. One question I did not answer there is this: why does Lloyd’s Algorithm work? Why should it converge at all? In this post I will attempt to answer this question.
 Cost Function K-means minimizes the within cluster sums of squares. We can define a function to describe this cost:
library(tidyverse) # point and mu are numerical vectors of the same length sum_of_squares = function(point,mu){ sum((point - mu)^2) } # points a data.</description>
    </item>
    
    <item>
      <title>An Implementation of K-means (Lloyd&#39;s Algorithm)</title>
      <link>/2018/10/01/an-implementation-of-k-means/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/01/an-implementation-of-k-means/</guid>
      <description>Introduction In this post I will write my own implementation of K-means. K-means is a popular clustering algorithm. You can read about it here: https://en.wikipedia.org/wiki/K-means_clustering.
 Algorithm Here is the description of the steps of the algorithm:
Choose k initial centers (or ‘centroids’). This can be done in many ways: chosen randomly from random uniform distributions, chosen randomly from the data, and other methods including k-means++, which I will discuss in a future post.</description>
    </item>
    
    <item>
      <title>My first post</title>
      <link>/2018/09/26/my-first-post/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/26/my-first-post/</guid>
      <description>My cool blog post.</description>
    </item>
    
  </channel>
</rss>